{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point \n",
    "import os \n",
    "from pathlib import Path\n",
    "import sys\n",
    "from tqdm import tqdm  \n",
    "\n",
    "# Define project root based on notebook location\n",
    "def notebook_find_project_root(current: Path, marker: str = \".git\"):\n",
    "    for parent in current.resolve().parents:\n",
    "        if (parent / marker).exists():\n",
    "            return parent\n",
    "    return current.resolve()  # fallback\n",
    "\n",
    "PROJECT_ROOT = notebook_find_project_root(Path.cwd())\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "INTERIM_DIR = PROJECT_ROOT / \"data\" / \"interim\"\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "EXTERNAL_DIR = PROJECT_ROOT / \"data\" / \"external\"\n",
    "IN_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"INDONESIA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in excel file and define future save directory\n",
    "xls_path = RAW_DIR / \"IN_DENGUE\" / \"DENGUE MONTHLY DATA (Updated).xlsx\"\n",
    "# Read in Indonesia shapefile\n",
    "in_shp = EXTERNAL_DIR / \"in_shp\" / \"Simplify_IDN\"\n",
    "# Read in Indonesia shapefile\n",
    "in_shp = gpd.read_file(in_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ID_2 values you want to highlight\n",
    "highlight_ids = [123, 125, 153, 204, 240, 379]  # example list\n",
    "\n",
    "# Separate GeoDataFrames\n",
    "highlighted = in_shp[in_shp['ID_2'].isin(highlight_ids)]\n",
    "rest = in_shp[~in_shp['ID_2'].isin(highlight_ids)]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "rest.plot(ax=ax, color='lightgrey', edgecolor='white')\n",
    "highlighted.plot(ax=ax, color='red', edgecolor='black')\n",
    "\n",
    "# Add labels (optional)\n",
    "for x, y, label in zip(highlighted.geometry.centroid.x, \n",
    "                       highlighted.geometry.centroid.y, \n",
    "                       highlighted['ID_2']):\n",
    "    ax.text(x, y, str(label), fontsize=9, ha='center', va='center', color='black')\n",
    "\n",
    "ax.set_title(\"Highlighted ID_2 Areas\")\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created: dengue_dubious_rows\n",
      "DataFrame created: dengue_normalized_discrepancies\n",
      "DataFrame created: dmi\n",
      "DataFrame created: dmi_east\n",
      "DataFrame created: enso_sst\n",
      "DataFrame created: era_chirps_id_1993_2023\n",
      "DataFrame created: lulc_200_full\n"
     ]
    }
   ],
   "source": [
    "# Loop through files and process CSV files\n",
    "for file in os.listdir(INTERIM_DIR):\n",
    "    if file.endswith('.csv'):\n",
    "        # Create the variable name based on the file name (without extension)\n",
    "        dataframe_name = file.split('.')[0].lower()\n",
    "        \n",
    "        # Load the CSV file into a DataFrame\n",
    "        file_path = os.path.join(INTERIM_DIR, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Assign the DataFrame to the variable name in globals\n",
    "        globals()[dataframe_name] = df\n",
    "        \n",
    "        # Print out the name of the dataframe and check the dataframe content\n",
    "        print(f'DataFrame created: {dataframe_name}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "era_1013 = pd.read_csv(r'D:\\Projects\\TMU\\gee_dengue\\zonal statistic\\era_missing_2010_2013.csv')\n",
    "era_1417 = pd.read_csv(r'D:\\Projects\\TMU\\gee_dengue\\zonal statistic\\era_missing_2014_2017.csv')\n",
    "era_18 = pd.read_csv(r'D:\\Projects\\TMU\\gee_dengue\\zonal statistic\\era_missing_2018_2021.csv')\n",
    "era_1821 = pd.read_csv(r'D:\\Projects\\TMU\\gee_dengue\\zonal statistic\\era_missing_2018aug_2021.csv')\n",
    "era_2223 = pd.read_csv(r'D:\\Projects\\TMU\\gee_dengue\\zonal statistic\\era_2022_2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all dataframes\n",
    "era_id_132 = pd.concat([era_1013, era_1417, era_18, era_1821, era_2223], ignore_index=True)\n",
    "\n",
    "# # Drop rows with any null values\n",
    "# era_id_132 = era_id_132.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_id_132['ID_2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Find rows with null values in 'temperature_2m_MEAN'\n",
    "null_temp_rows = era_chirps_id_1993_2023[era_chirps_id_1993_2023['temperature_2m_MEAN'].isnull()]\n",
    "print(f\"Number of rows with nulls in 'temperature_2m_MEAN': {len(null_temp_rows)}\")\n",
    "print(null_temp_rows)\n",
    "\n",
    "# Find rows with null values in 'precipitation_MEAN'\n",
    "null_precip_rows = era_chirps_id_1993_2023[era_chirps_id_1993_2023['precipitation_MEAN'].isnull()]\n",
    "print(f\"\\nNumber of rows with nulls in 'precipitation_MEAN': {len(null_precip_rows)}\")\n",
    "print(null_precip_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_temp_rows['ID_2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_precip_rows['ID_2'].unique()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First drop the 'Unnamed: 0' column from the era_chirps_id_1993_2023 DataFrame\n",
    "era_chirps_id_1993_2023 = era_chirps_id_1993_2023.drop(columns=['Unnamed: 0'])\n",
    "null_rows_era_chirps = era_chirps_id_1993_2023[era_chirps_id_1993_2023.isnull().any(axis=1)]\n",
    "# Display rows with null values (if any)\n",
    "print(f\"Number of rows with nulls: {len(null_rows_era_chirps)}\")\n",
    "print(null_rows_era_chirps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows_era_chirps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'precipitation_MEAN' is null\n",
    "null_precipitation_ids = era_chirps_id_1993_2023[era_chirps_id_1993_2023['temperature_2m_MEAN'].isnull()]['ID_2'].unique()\n",
    "\n",
    "# Display the unique ID_2 values\n",
    "print(\"ID_2 values with null precipitation_MEAN:\")\n",
    "print(null_precipitation_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows_era_chirps['ID_2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_chirps_id_1993_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows_era_chirps['precipitation_MEAN'].isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legacy code for discovering underrepresented ID_2 in climate variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_incomplete_ids(df, id_column='ID_2'):\n",
    "    \"\"\"\n",
    "    Finds and returns IDs associated with rows that have any null values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to check.\n",
    "    id_column (str): The column containing the ID values.\n",
    "\n",
    "    Returns:\n",
    "    list: IDs of rows with null values.\n",
    "    \"\"\"\n",
    "    incomplete_ids = df[df.isnull().any(axis=1)][id_column].unique().tolist()\n",
    "\n",
    "    print(f\"Total rows with nulls: {df.isnull().any(axis=1).sum()}\")\n",
    "    print(f\"Unique {id_column}s with nulls: {len(incomplete_ids)} -> {incomplete_ids}\")\n",
    "\n",
    "    return incomplete_ids\n",
    "\n",
    "def fill_incomplete_polygons(dataframe, shapefile, incomplete_ids, stat_cols):\n",
    "    \"\"\"\n",
    "    Fill rows with nulls by copying values from the nearest complete polygon.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): Main DataFrame with 'ID_2' and data.\n",
    "        shapefile (gpd.GeoDataFrame): GeoDataFrame with 'ID_2' and geometries.\n",
    "        incomplete_ids (list): List of 'ID_2' values with incomplete data.\n",
    "        stat_cols (list): Statistic columns to fill (optional).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with filled-in rows.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure shapefile uses a projected CRS\n",
    "    shapefile = shapefile.to_crs(epsg=3857)\n",
    "    shapefile[\"centroid\"] = shapefile.geometry.centroid\n",
    "\n",
    "    # Get valid (complete) polygons\n",
    "    valid_ids = dataframe[~dataframe[\"ID_2\"].isin(incomplete_ids)][\"ID_2\"].unique()\n",
    "    non_missing_polygons = shapefile[shapefile[\"ID_2\"].isin(valid_ids)].copy()\n",
    "\n",
    "    # Map each incomplete ID to its nearest valid neighbor\n",
    "    nearest_mapping = {}\n",
    "\n",
    "    print(\"Finding nearest complete polygons for incomplete ID_2 values...\")\n",
    "    for ID in tqdm(incomplete_ids, desc=\"Computing nearest polygons\"):\n",
    "        centroid = shapefile[shapefile[\"ID_2\"] == ID].centroid.iloc[0]\n",
    "        non_missing_polygons[\"distance\"] = non_missing_polygons[\"centroid\"].distance(centroid)\n",
    "        nearest_mapping[ID] = non_missing_polygons.loc[non_missing_polygons[\"distance\"].idxmin(), \"ID_2\"]\n",
    "\n",
    "    # Fill the incomplete rows\n",
    "    updated_rows = []\n",
    "    print(\"\\nFilling incomplete data using nearest neighbors...\")\n",
    "    for ID in tqdm(incomplete_ids, desc=\"Filling rows\"):\n",
    "        original_rows = dataframe[dataframe[\"ID_2\"] == ID]\n",
    "        nearest_ID = nearest_mapping[ID]\n",
    "        fill_source = dataframe[dataframe[\"ID_2\"] == nearest_ID].iloc[0]\n",
    "\n",
    "        for _, row in original_rows.iterrows():\n",
    "            new_row = row.copy()\n",
    "            for col in dataframe.columns:\n",
    "                if pd.isnull(new_row[col]):\n",
    "                    new_row[col] = fill_source[col]\n",
    "            updated_rows.append(new_row)\n",
    "\n",
    "    # Remove old incomplete rows and append updated ones\n",
    "    clean_df = dataframe[~dataframe[\"ID_2\"].isin(incomplete_ids)]\n",
    "    updated_df = pd.concat([clean_df, pd.DataFrame(updated_rows)], ignore_index=True)\n",
    "\n",
    "    print(f\"\\n✅ Incomplete data filled for {len(incomplete_ids)} ID_2 entries.\")\n",
    "    return updated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows with nulls: 0\n",
      "Unique ID_2s with nulls: 0 -> []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_incomplete_ids(era_chirps_id_1993_2023_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_chirps_id_1993_2023_filled.to_csv(INTERIM_DIR/ \"era_chirps_id_1993_2023.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_incomplete_ids(era_chirps_id_1993_2023, id_column='ID_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding nearest complete polygons for incomplete ID_2 values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing nearest polygons: 100%|██████████| 6/6 [00:00<00:00, 1435.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filling incomplete data using nearest neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filling rows: 100%|██████████| 6/6 [00:11<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Incomplete data filled for 6 ID_2 entries.\n"
     ]
    }
   ],
   "source": [
    "era_chirps_id_1993_2023_filled = fill_incomplete_polygons(dataframe=era_chirps_id_1993_2023,\n",
    "    shapefile=in_shp, incomplete_ids=[123, 125, 153, 204, 240, 379], stat_cols=['temperature_2m_MEAN', 'temperature_2m_min_MEAN',\n",
    "       'temperature_2m_max_MEAN', 'potential_evaporation_sum_MEAN',\n",
    "       'total_evaporation_sum_MEAN', 'precipitation_MEAN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Doy</th>\n",
       "      <th>ID_2</th>\n",
       "      <th>temperature_2m_MEAN</th>\n",
       "      <th>temperature_2m_min_MEAN</th>\n",
       "      <th>temperature_2m_max_MEAN</th>\n",
       "      <th>potential_evaporation_sum_MEAN</th>\n",
       "      <th>total_evaporation_sum_MEAN</th>\n",
       "      <th>Region</th>\n",
       "      <th>precipitation_MEAN</th>\n",
       "      <th>YearMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.709194</td>\n",
       "      <td>16.977877</td>\n",
       "      <td>24.706811</td>\n",
       "      <td>-0.010296</td>\n",
       "      <td>-0.003956</td>\n",
       "      <td>Sumatra</td>\n",
       "      <td>2.037670</td>\n",
       "      <td>1993-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>21.582465</td>\n",
       "      <td>18.095360</td>\n",
       "      <td>25.474377</td>\n",
       "      <td>-0.009332</td>\n",
       "      <td>-0.004078</td>\n",
       "      <td>Sumatra</td>\n",
       "      <td>0.262112</td>\n",
       "      <td>1993-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>24.692348</td>\n",
       "      <td>22.235172</td>\n",
       "      <td>27.720153</td>\n",
       "      <td>-0.011643</td>\n",
       "      <td>-0.004529</td>\n",
       "      <td>Sumatra</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1993-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>23.722732</td>\n",
       "      <td>20.910947</td>\n",
       "      <td>27.319734</td>\n",
       "      <td>-0.010711</td>\n",
       "      <td>-0.004267</td>\n",
       "      <td>Sumatra</td>\n",
       "      <td>0.748526</td>\n",
       "      <td>1993-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>20.802229</td>\n",
       "      <td>17.112204</td>\n",
       "      <td>24.707693</td>\n",
       "      <td>-0.010373</td>\n",
       "      <td>-0.003945</td>\n",
       "      <td>Sumatra</td>\n",
       "      <td>3.548272</td>\n",
       "      <td>1993-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026963</th>\n",
       "      <td>5026963</td>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>361</td>\n",
       "      <td>444</td>\n",
       "      <td>27.313731</td>\n",
       "      <td>24.367449</td>\n",
       "      <td>31.377478</td>\n",
       "      <td>-0.008531</td>\n",
       "      <td>-0.004548</td>\n",
       "      <td>Java</td>\n",
       "      <td>13.263003</td>\n",
       "      <td>2023-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026964</th>\n",
       "      <td>5026964</td>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>362</td>\n",
       "      <td>444</td>\n",
       "      <td>26.902143</td>\n",
       "      <td>23.847837</td>\n",
       "      <td>31.970732</td>\n",
       "      <td>-0.008388</td>\n",
       "      <td>-0.004595</td>\n",
       "      <td>Java</td>\n",
       "      <td>13.246793</td>\n",
       "      <td>2023-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026965</th>\n",
       "      <td>5026965</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>363</td>\n",
       "      <td>444</td>\n",
       "      <td>26.763019</td>\n",
       "      <td>23.781351</td>\n",
       "      <td>31.827548</td>\n",
       "      <td>-0.008483</td>\n",
       "      <td>-0.005088</td>\n",
       "      <td>Java</td>\n",
       "      <td>18.036463</td>\n",
       "      <td>2023-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026966</th>\n",
       "      <td>5026966</td>\n",
       "      <td>2023-12-30</td>\n",
       "      <td>364</td>\n",
       "      <td>444</td>\n",
       "      <td>26.403595</td>\n",
       "      <td>24.242135</td>\n",
       "      <td>30.929505</td>\n",
       "      <td>-0.006610</td>\n",
       "      <td>-0.004357</td>\n",
       "      <td>Java</td>\n",
       "      <td>12.833191</td>\n",
       "      <td>2023-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026967</th>\n",
       "      <td>5026967</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>365</td>\n",
       "      <td>444</td>\n",
       "      <td>26.428799</td>\n",
       "      <td>23.772490</td>\n",
       "      <td>31.288547</td>\n",
       "      <td>-0.007260</td>\n",
       "      <td>-0.004578</td>\n",
       "      <td>Java</td>\n",
       "      <td>12.271276</td>\n",
       "      <td>2023-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5026968 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0        Date  Doy  ID_2  temperature_2m_MEAN  \\\n",
       "0                 0  1993-01-01    1     1            20.709194   \n",
       "1                 1  1993-01-01    1     2            21.582465   \n",
       "2                 2  1993-01-01    1     3            24.692348   \n",
       "3                 3  1993-01-01    1     4            23.722732   \n",
       "4                 4  1993-01-01    1     5            20.802229   \n",
       "...             ...         ...  ...   ...                  ...   \n",
       "5026963     5026963  2023-12-27  361   444            27.313731   \n",
       "5026964     5026964  2023-12-28  362   444            26.902143   \n",
       "5026965     5026965  2023-12-29  363   444            26.763019   \n",
       "5026966     5026966  2023-12-30  364   444            26.403595   \n",
       "5026967     5026967  2023-12-31  365   444            26.428799   \n",
       "\n",
       "         temperature_2m_min_MEAN  temperature_2m_max_MEAN  \\\n",
       "0                      16.977877                24.706811   \n",
       "1                      18.095360                25.474377   \n",
       "2                      22.235172                27.720153   \n",
       "3                      20.910947                27.319734   \n",
       "4                      17.112204                24.707693   \n",
       "...                          ...                      ...   \n",
       "5026963                24.367449                31.377478   \n",
       "5026964                23.847837                31.970732   \n",
       "5026965                23.781351                31.827548   \n",
       "5026966                24.242135                30.929505   \n",
       "5026967                23.772490                31.288547   \n",
       "\n",
       "         potential_evaporation_sum_MEAN  total_evaporation_sum_MEAN   Region  \\\n",
       "0                             -0.010296                   -0.003956  Sumatra   \n",
       "1                             -0.009332                   -0.004078  Sumatra   \n",
       "2                             -0.011643                   -0.004529  Sumatra   \n",
       "3                             -0.010711                   -0.004267  Sumatra   \n",
       "4                             -0.010373                   -0.003945  Sumatra   \n",
       "...                                 ...                         ...      ...   \n",
       "5026963                       -0.008531                   -0.004548     Java   \n",
       "5026964                       -0.008388                   -0.004595     Java   \n",
       "5026965                       -0.008483                   -0.005088     Java   \n",
       "5026966                       -0.006610                   -0.004357     Java   \n",
       "5026967                       -0.007260                   -0.004578     Java   \n",
       "\n",
       "         precipitation_MEAN YearMonth  \n",
       "0                  2.037670   1993-01  \n",
       "1                  0.262112   1993-01  \n",
       "2                  0.000000   1993-01  \n",
       "3                  0.748526   1993-01  \n",
       "4                  3.548272   1993-01  \n",
       "...                     ...       ...  \n",
       "5026963           13.263003   2023-12  \n",
       "5026964           13.246793   2023-12  \n",
       "5026965           18.036463   2023-12  \n",
       "5026966           12.833191   2023-12  \n",
       "5026967           12.271276   2023-12  \n",
       "\n",
       "[5026968 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "era_chirps_id_1993_2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up era-chirps column name and convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First drop the 'Unnamed: 0' column from the era_chirps_id_1993_2023 DataFrame\n",
    "era_chirps_id_1993_2023 = era_chirps_id_1993_2023.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Remove redundant suffixes from column names\n",
    "columns_to_check = [\n",
    "    'temperature_2m_MEAN', 'temperature_2m_min_MEAN',\n",
    "    'temperature_2m_max_MEAN', 'potential_evaporation_sum_MEAN',\n",
    "    'total_evaporation_sum_MEAN', 'precipitation_MEAN'\n",
    "]\n",
    "# Create a dictionary for renaming\n",
    "rename_dict = {col: col.replace('_MEAN', '') for col in era_chirps_id_1993_2023.columns if col in columns_to_check and '_MEAN' in col}\n",
    "era_chirps_id_1993_2023.rename(columns=rename_dict, inplace=True)\n",
    "# Convert total evaporation and potential evaporation to positive values (ERA5 convention-->standard\n",
    "# meteorological convention)\n",
    "era_chirps_id_1993_2023['total_evaporation_sum'] = era_chirps_id_1993_2023['total_evaporation_sum'].abs()\n",
    "era_chirps_id_1993_2023['potential_evaporation_sum'] = era_chirps_id_1993_2023['potential_evaporation_sum'].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ID_2, daily statisitcs and evaporative stress index + aridity index. (NO LULC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the era_chirps_id_1993_2023 DataFrame to avoid modifying the original\n",
    "env_var = era_chirps_id_1993_2023.copy()\n",
    "# Make sure the 'Date' column is in datetime format\n",
    "env_var['Date'] = pd.to_datetime(env_var['Date'])\n",
    "# Create a 'YearMonth' column for merging\n",
    "env_var['YearMonth'] = env_var['Date'].dt.to_period('M')\n",
    "# Convert YearMonth to timestamp for merging\n",
    "env_var['YearMonth'] = env_var['YearMonth'].dt.to_timestamp()\n",
    "\n",
    "# Create a matching 'YearMonth' column in enso dataframe\n",
    "enso_sst['YearMonth'] = pd.to_datetime(enso_sst['Year'].astype(str) + '-' + enso_sst['Month'].astype(str).str.zfill(2))\n",
    "\n",
    "# Columns to add from enso\n",
    "enso_columns_to_add = ['YearMonth', 'NINO1+2', 'ANOM1+2', 'NINO3', 'ANOM3', 'NINO4', 'ANOM4', 'NINO3.4', 'ANOM3.4']\n",
    "\n",
    "# Merge enso dataframes directly with env_var\n",
    "merged_df = env_var.merge(enso_sst[enso_columns_to_add], on='YearMonth', how='left')\n",
    "\n",
    "# First, melt dmi dataframe (convert from wide to long format)\n",
    "dmi_long = dmi.melt(id_vars='Year', var_name='Month', value_name='DMI')\n",
    "dmi_long['Month'] = dmi_long['Month'].astype(int)  # ensure Month is integer\n",
    "dmi_long['YearMonth'] = pd.to_datetime(dmi_long['Year'].astype(str) + '-' + dmi_long['Month'].astype(str).str.zfill(2))\n",
    "\n",
    "# Melt dmi_east dataframe\n",
    "dmi_east_long = dmi_east.melt(id_vars='Year', var_name='Month', value_name='DMI_East')\n",
    "dmi_east_long['Month'] = dmi_east_long['Month'].astype(int)\n",
    "dmi_east_long['YearMonth'] = pd.to_datetime(dmi_east_long['Year'].astype(str) + '-' + dmi_east_long['Month'].astype(str).str.zfill(2))\n",
    "\n",
    "# Merge DMI\n",
    "merged_df = merged_df.merge(dmi_long[['YearMonth', 'DMI']], on='YearMonth', how='left')\n",
    "# Merge DMI East\n",
    "merged_df = merged_df.merge(dmi_east_long[['YearMonth', 'DMI_East']], on='YearMonth', how='left')\n",
    "\n",
    "# Calculate evaporative stress index and aridity index\n",
    "merged_df['evaporative_stress_index'] = merged_df['total_evaporation_sum']/merged_df['potential_evaporation_sum']\n",
    "merged_df['aridity_index'] = merged_df['precipitation'] / merged_df['potential_evaporation_sum']\n",
    "\n",
    "\n",
    "# Save full merged dataframe\n",
    "merged_df.to_csv(IN_DIR / 'daily_env_id_1993_2023.csv', index=False)\n",
    "# Filter the dataframe for the years 2010 to 2023\n",
    "filtered_df = merged_df[(merged_df['YearMonth'].dt.year >= 2010) & (merged_df['YearMonth'].dt.year <= 2023)].reset_index(drop=True)\n",
    "filtered_df.to_csv(IN_DIR / 'daily_env_id_2010_2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating ID_2, monthly statistics. (+LULC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.copy()\n",
    "# Ensure Date column is datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "# Drop 'Doy' and replace 'YearMonth' with first day of month\n",
    "df['YearMonth'] = df['Date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Define relevant column sets\n",
    "temperature_cols = ['temperature_2m', 'temperature_2m_min', 'temperature_2m_max']\n",
    "evap_precip_cols = ['potential_evaporation_sum', 'total_evaporation_sum', 'precipitation']\n",
    "monthly_cols = ['NINO1+2', 'ANOM1+2', 'NINO3', 'ANOM3', 'NINO4', 'ANOM4', 'NINO3.4',\n",
    "                'ANOM3.4', 'DMI', 'DMI_East']\n",
    "keep_cols = ['YearMonth', 'ID_2', 'Region'] + temperature_cols + evap_precip_cols + monthly_cols\n",
    "\n",
    "# Filter necessary columns\n",
    "df = df[keep_cols]\n",
    "\n",
    "# Group by YearMonth and ID_2\n",
    "monthly_df = df.groupby(['YearMonth', 'ID_2'], as_index=False).agg({\n",
    "    **{col: 'mean' for col in temperature_cols},\n",
    "    **{col: 'sum' for col in evap_precip_cols},\n",
    "    **{col: 'first' for col in monthly_cols},  # Since these are already monthly\n",
    "    'Region': 'first'  # Assuming region does not change within ID_2\n",
    "})\n",
    "\n",
    "# Recompute evaporative_stress_index and aridity_index\n",
    "monthly_df['evaporative_stress_index'] = (\n",
    "    monthly_df['total_evaporation_sum'] / monthly_df['potential_evaporation_sum'])\n",
    "monthly_df['aridity_index'] = (\n",
    "    monthly_df['precipitation'] / monthly_df['potential_evaporation_sum'])\n",
    "\n",
    "# Add lulc class columns\n",
    "# 1. Create a mapping of ID_2 to Region from dengue_by_ID_2\n",
    "id2_region_map = era_chirps_id_1993_2023[['ID_2', 'Region']].drop_duplicates()\n",
    "# 2. Add 'Region' column to lulc_final\n",
    "lulc_200_full = pd.merge(lulc_200_full, id2_region_map, on='ID_2', how='left')\n",
    "\n",
    "# Define LULC class columns\n",
    "class_columns = [\n",
    "    'Class_70', 'Class_60', 'Class_50', 'Class_40', 'Class_95',\n",
    "    'Class_30', 'Class_20', 'Class_10', 'Class_90', 'Class_80'\n",
    "]\n",
    "\n",
    "# Compute True_Area_* columns based on class proportions and Class_sum\n",
    "for col in class_columns:\n",
    "    lulc_200_full[f'True_Area_{col}'] = lulc_200_full[col] * lulc_200_full['Class_sum']\n",
    "\n",
    "# Select only the necessary columns from lulc_200_full\n",
    "true_area_cols = [f'True_Area_{col}' for col in class_columns]\n",
    "all_lulc_columns = ['ID_2', 'Class_sum'] + class_columns + true_area_cols\n",
    "\n",
    "# Merge into monthly_df\n",
    "monthly_df = pd.merge(monthly_df, lulc_200_full[all_lulc_columns], on='ID_2', how='left')\n",
    "monthly_df.to_csv(IN_DIR / 'monthly_env_id_1993_2023.csv', index=False)\n",
    "\n",
    "filtered_monthly_df = monthly_df[(monthly_df['YearMonth'].dt.year >= 2010) & \n",
    "                                  (monthly_df['YearMonth'].dt.year <= 2023)].reset_index(drop=True)\n",
    "filtered_monthly_df.to_csv(IN_DIR / 'monthly_env_id_2010_2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning daily statistics to monthly statistics -- BY REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataframe is named 'env_var'\n",
    "env_var = era_chirps_id_1993_2023.copy()\n",
    "# Make sure the 'Date' column is in datetime format\n",
    "env_var['Date'] = pd.to_datetime(env_var['Date'])\n",
    "# Create a 'YearMonth' column for grouping\n",
    "env_var['YearMonth'] = env_var['Date'].dt.to_period('M')\n",
    "\n",
    "# Group by 'Region' and 'YearMonth', then calculate the mean for relevant columns\n",
    "monthly_env_mean = env_var.groupby(['Region', 'YearMonth'])[\n",
    "    ['temperature_2m', 'temperature_2m_min', 'temperature_2m_max']\n",
    "].mean().reset_index()\n",
    "\n",
    "# Group by 'Region' and 'YearMonth', then calculate the sum for relevant columns\n",
    "monthly_env_sum = env_var.groupby(['Region', 'YearMonth'])[\n",
    "    ['precipitation','potential_evaporation_sum', 'total_evaporation_sum']\n",
    "].sum().reset_index()\n",
    "\n",
    "# Merge the mean and sum dataframes\n",
    "monthly_env_data = pd.merge(monthly_env_mean, monthly_env_sum, on=['Region', 'YearMonth'], how='inner')\n",
    "\n",
    "# If YearMonth is a Period (like Period('2023-01', 'M')), convert it to timestamp\n",
    "monthly_env_data['YearMonth'] = monthly_env_data['YearMonth'].dt.to_timestamp()\n",
    "\n",
    "# Create a matching 'YearMonth' column in enso dataframe\n",
    "enso_sst['YearMonth'] = pd.to_datetime(enso_sst['Year'].astype(str) + '-' + enso_sst['Month'].astype(str).str.zfill(2))\n",
    "\n",
    "# Columns to add from enso\n",
    "enso_columns_to_add = ['YearMonth', 'NINO1+2', 'ANOM1+2', 'NINO3', 'ANOM3', 'NINO4', 'ANOM4', 'NINO3.4', 'ANOM3.4']\n",
    "\n",
    "# Merge enso dataframes\n",
    "merged_df = monthly_env_data.merge(enso_sst[enso_columns_to_add], on='YearMonth', how='left')\n",
    "\n",
    "# First, melt dmi dataframe (convert from wide to long format)\n",
    "dmi_long = dmi.melt(id_vars='Year', var_name='Month', value_name='DMI')\n",
    "dmi_long['Month'] = dmi_long['Month'].astype(int)  # ensure Month is integer\n",
    "dmi_long['YearMonth'] = pd.to_datetime(dmi_long['Year'].astype(str) + '-' + dmi_long['Month'].astype(str).str.zfill(2))\n",
    "\n",
    "# Melt dmi_east dataframe\n",
    "dmi_east_long = dmi_east.melt(id_vars='Year', var_name='Month', value_name='DMI_East')\n",
    "dmi_east_long['Month'] = dmi_east_long['Month'].astype(int)\n",
    "dmi_east_long['YearMonth'] = pd.to_datetime(dmi_east_long['Year'].astype(str) + '-' + dmi_east_long['Month'].astype(str).str.zfill(2))\n",
    "\n",
    "# Merge DMI\n",
    "merged_df = merged_df.merge(dmi_long[['YearMonth', 'DMI']], on='YearMonth', how='left')\n",
    "# Merge DMI East\n",
    "merged_df = merged_df.merge(dmi_east_long[['YearMonth', 'DMI_East']], on='YearMonth', how='left')\n",
    "\n",
    "\n",
    "# 1. Create a mapping of ID_2 to Region from dengue_by_ID_2\n",
    "id2_region_map = era_chirps_id_1993_2023[['ID_2', 'Region']].drop_duplicates()\n",
    "# 2. Add 'Region' column to lulc_final\n",
    "lulc_200_full = pd.merge(lulc_200_full, id2_region_map, on='ID_2', how='left')\n",
    "\n",
    "# Define the list of columns to aggregate\n",
    "area_columns = [\n",
    "    'True_Area_Class_70', 'True_Area_Class_60', 'True_Area_Class_50',\n",
    "    'True_Area_Class_40', 'True_Area_Class_95', 'True_Area_Class_30',\n",
    "    'True_Area_Class_20', 'True_Area_Class_10', 'True_Area_Class_90',\n",
    "    'True_Area_Class_80', 'Class_sum'\n",
    "]\n",
    "class_columns = ['Class_70', 'Class_60', 'Class_50', 'Class_40', 'Class_95', \n",
    "                 'Class_30', 'Class_20', 'Class_10', 'Class_90', 'Class_80']\n",
    "for col in class_columns:\n",
    "    lulc_200_full[f'True_Area_{col}'] = lulc_200_full[col] * lulc_200_full['Class_sum']\n",
    "\n",
    "# 3. Aggregate lulc_final_with_region by 'Region'\n",
    "# We sum the area columns to get total area per region for each class\n",
    "lulc_agg_by_region = lulc_200_full.groupby('Region')[area_columns].sum().reset_index()\n",
    "\n",
    "# 4. Merge the aggregated LULC data into monthly_dengue_env_region\n",
    "# Since LULC data is static (not time-series in this context), it's merged only on 'Region'\n",
    "merged_df = pd.merge(\n",
    "    merged_df,\n",
    "    lulc_agg_by_region,\n",
    "    on='Region',\n",
    "    how='inner' # Use 'inner' to ensure only matching regions are kept\n",
    ")\n",
    "\n",
    "true_area_class_columns = [\n",
    "    'True_Area_Class_70', 'True_Area_Class_60', 'True_Area_Class_50',\n",
    "    'True_Area_Class_40', 'True_Area_Class_95', 'True_Area_Class_30',\n",
    "    'True_Area_Class_20', 'True_Area_Class_10', 'True_Area_Class_90',\n",
    "    'True_Area_Class_80']\n",
    "# Create new percentage columns\n",
    "for col in true_area_class_columns:\n",
    "    # Extract the class number from the column name (e.g., '70' from 'True_Area_Class_70')\n",
    "    class_num = col.replace('True_Area_Class_', '')\n",
    "    new_col_name = f'Class_{class_num}'\n",
    "    # Calculate the percentage. Handle potential division by zero by checking if 'Class_sum' is 0\n",
    "    # If Class_sum is 0, the percentage for that row will be 0.\n",
    "    merged_df[new_col_name] = merged_df.apply(\n",
    "        lambda row: row[col] / row['Class_sum'] if row['Class_sum'] != 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Recompute evaporative_stress_index and aridity_index\n",
    "merged_df['evaporative_stress_index'] = (\n",
    "    merged_df['total_evaporation_sum'] / merged_df['potential_evaporation_sum'])\n",
    "merged_df['aridity_index'] = (\n",
    "    merged_df['precipitation'] / merged_df['potential_evaporation_sum'])\n",
    "# Save the final merged dataframe\n",
    "merged_df.to_csv(IN_DIR / 'monthly_env_region_1993_2023.csv', index=False)\n",
    "\n",
    "# Filter the dataframe\n",
    "filtered_df = merged_df[(merged_df['YearMonth'].dt.year >= 2010) & (merged_df['YearMonth'].dt.year <= 2023)].reset_index(drop=True)\n",
    "filtered_df.to_csv(IN_DIR / 'monthly_env_region_2010_2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "National Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all True_Area columns for national summary of lulc percentages\n",
    "true_area_cols = [col for col in lulc_200_full.columns if col.startswith(\"True_Area_Class_\")]\n",
    "# Sum true area for each class\n",
    "true_area_totals = lulc_200_full[true_area_cols].sum()\n",
    "# Total area across all classes\n",
    "total_area = true_area_totals.sum()\n",
    "# Build proportion columns named Class_{i}\n",
    "class_proportions = {\n",
    "    f\"Class_{col.split('_')[-1]}\": (true_area_totals[col] / total_area).round(6)\n",
    "    for col in true_area_cols\n",
    "}\n",
    "# Combine all into one row\n",
    "summary_row = {\n",
    "    **class_proportions,\n",
    "    **true_area_totals.to_dict(),\n",
    "    \"Class_sum\": total_area\n",
    "}\n",
    "# Create DataFrame\n",
    "lulc_200_national = pd.DataFrame([summary_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataframe is named 'env_var'\n",
    "env_var = era_chirps_id_1993_2023.copy()\n",
    "# Make sure the 'Date' column is in datetime format\n",
    "env_var['Date'] = pd.to_datetime(env_var['Date'])\n",
    "\n",
    "# Create a 'YearMonth' column for grouping\n",
    "env_var['YearMonth'] = env_var['Date'].dt.to_period('M')\n",
    "\n",
    "# Group by 'Region' and 'YearMonth', then calculate the mean for relevant columns\n",
    "monthly_env_mean = env_var.groupby(['YearMonth'])[\n",
    "    ['temperature_2m', 'temperature_2m_min', 'temperature_2m_max']\n",
    "].mean().reset_index()\n",
    "\n",
    "# Group by 'Region' and 'YearMonth', then calculate the sum for relevant columns\n",
    "monthly_env_sum = env_var.groupby(['YearMonth'])[\n",
    "    ['precipitation','potential_evaporation_sum', 'total_evaporation_sum']\n",
    "].sum().reset_index()\n",
    "\n",
    "# Merge the mean and sum dataframes\n",
    "monthly_env_data = pd.merge(monthly_env_mean, monthly_env_sum, on=['YearMonth'], how='inner')\n",
    "\n",
    "# If YearMonth is a Period (like Period('2023-01', 'M')), convert it to timestamp\n",
    "monthly_env_data['YearMonth'] = monthly_env_data['YearMonth'].dt.to_timestamp()\n",
    "\n",
    "# Create a matching 'YearMonth' column in enso dataframe\n",
    "enso_sst['YearMonth'] = pd.to_datetime(enso_sst['Year'].astype(str) + '-' + enso_sst['Month'].astype(str).str.zfill(2))\n",
    "\n",
    "# Columns to add from enso\n",
    "enso_columns_to_add = ['YearMonth', 'NINO1+2', 'ANOM1+2', 'NINO3', 'ANOM3', 'NINO4', 'ANOM4', 'NINO3.4', 'ANOM3.4']\n",
    "\n",
    "# Merge enso dataframes\n",
    "merged_df = monthly_env_data.merge(enso_sst[enso_columns_to_add], on='YearMonth', how='left')\n",
    "\n",
    "# First, melt dmi dataframe (convert from wide to long format)\n",
    "dmi_long = dmi.melt(id_vars='Year', var_name='Month', value_name='DMI')\n",
    "dmi_long['Month'] = dmi_long['Month'].astype(int)  # ensure Month is integer\n",
    "dmi_long['YearMonth'] = pd.to_datetime(dmi_long['Year'].astype(str) + '-' + dmi_long['Month'].astype(str).str.zfill(2))\n",
    "\n",
    "# Melt dmi_east dataframe\n",
    "dmi_east_long = dmi_east.melt(id_vars='Year', var_name='Month', value_name='DMI_East')\n",
    "dmi_east_long['Month'] = dmi_east_long['Month'].astype(int)\n",
    "dmi_east_long['YearMonth'] = pd.to_datetime(dmi_east_long['Year'].astype(str) + '-' + dmi_east_long['Month'].astype(str).str.zfill(2))\n",
    "\n",
    "# Merge DMI\n",
    "merged_df = merged_df.merge(dmi_long[['YearMonth', 'DMI']], on='YearMonth', how='left')\n",
    "# Merge DMI East\n",
    "merged_df = merged_df.merge(dmi_east_long[['YearMonth', 'DMI_East']], on='YearMonth', how='left')\n",
    "\n",
    "# Recompute evaporative_stress_index and aridity_index\n",
    "merged_df['evaporative_stress_index'] = (\n",
    "    merged_df['total_evaporation_sum'] / merged_df['potential_evaporation_sum'])\n",
    "merged_df['aridity_index'] = (\n",
    "    merged_df['precipitation'] / merged_df['potential_evaporation_sum'])\n",
    "\n",
    "# Repeat summary_df to match the number of rows in merged_df\n",
    "lulc_replicated = pd.concat([lulc_200_national] * len(merged_df), ignore_index=True)\n",
    "# Concatenate along columns\n",
    "merged_df = pd.concat([merged_df.reset_index(drop=True), lulc_replicated], axis=1)\n",
    "\n",
    "merged_df.to_csv(IN_DIR / 'monthly_env_national_1993_2023.csv', index=False)\n",
    "\n",
    "# Filter the dataframe\n",
    "filtered_df = merged_df[(merged_df['YearMonth'].dt.year >= 2010) & (merged_df['YearMonth'].dt.year <= 2023)].reset_index(drop=True)\n",
    "filtered_df.to_csv(IN_DIR / 'monthly_env_national_2010_2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_df = era_1993_2023.merge(in_shp[['ID_2', 'Region']], on='ID_2')\n",
    "era_df['Date'] = pd.to_datetime(era_df['Date'])\n",
    "era_df['Month'] = era_df['Date'].dt.month\n",
    "\n",
    "climatology_vars = ['temperature_2m_MEAN', 'temperature_2m_min_MEAN',\n",
    "       'temperature_2m_max_MEAN', 'potential_evaporation_sum_MEAN']\n",
    "era_climatology = era_df.groupby(['Region', 'Month'])[climatology_vars].mean().reset_index()\n",
    "era_merged = era_df.merge(era_climatology, on=['Region', 'Month'], suffixes=('', '_clim'))\n",
    "for var in climatology_vars:\n",
    "    era_merged[f'{var}_anomaly'] = era_merged[var] - era_merged[f'{var}_clim']\n",
    "era_anomaly = era_merged.groupby(['Region', 'Date'])[\n",
    "    [f'{var}_anomaly' for var in climatology_vars]\n",
    "].mean().reset_index()\n",
    "\n",
    "regions = era_anomaly['Region'].unique()\n",
    "for var in climatology_vars:\n",
    "    for region in regions:\n",
    "        subset = era_anomaly[era_anomaly['Region'] == region]\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        sns.lineplot(data=subset, x='Date', y=f'{var}_anomaly')\n",
    "        plt.title(f\"{var.capitalize()} Anomaly in {region} (1993–2023)\")\n",
    "        plt.ylabel(f\"{var.capitalize()} Anomaly\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save to FIGURE_DIR\n",
    "        filename = f\"{var}_anomaly_{region.replace(' ', '_')}.png\"\n",
    "        plt.savefig(os.path.join(FIGURE_DIR, filename), dpi=300)\n",
    "        plt.close()  # Close to avoid memory issues when looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_df = era_1993_2023.merge(in_shp[['ID_2', 'Region']], on='ID_2')\n",
    "era_df['Date'] = pd.to_datetime(era_df['Date'])\n",
    "era_df['Month'] = era_df['Date'].dt.month\n",
    "\n",
    "climatology_vars = ['temperature_2m_MEAN', 'temperature_2m_min_MEAN',\n",
    "       'temperature_2m_max_MEAN', 'potential_evaporation_sum_MEAN']\n",
    "era_climatology = era_df.groupby(['Region', 'Month'])[climatology_vars].mean().reset_index()\n",
    "era_merged = era_df.merge(era_climatology, on=['Region', 'Month'], suffixes=('', '_clim'))\n",
    "\n",
    "for var in climatology_vars:\n",
    "    era_merged[f'{var}_anomaly'] = era_merged[var] - era_merged[f'{var}_clim']\n",
    "era_anomaly = era_merged.groupby(['Region', 'Date'])[\n",
    "    [f'{var}_anomaly' for var in climatology_vars]\n",
    "].mean().reset_index()\n",
    "for var in climatology_vars:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.lineplot(data=era_anomaly, x='Date', y=f'{var}_anomaly', hue='Region')\n",
    "    plt.title(f\"ERA {var.capitalize()} Anomaly (1993–2023) All Regions\")\n",
    "    plt.ylabel(f\"{var.capitalize()} Anomaly\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    # Save to FIGURE_DIR\n",
    "    filename = f\"{var}_anomaly_all_regions.png\"\n",
    "    plt.savefig(os.path.join(FIGURE_DIR, filename), dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirsp_df = chirsp_1993_2023.merge(in_shp[['ID_2', 'Region']], on='ID_2')\n",
    "chirsp_df['Date'] = pd.to_datetime(chirsp_df['Date'])\n",
    "chirsp_df['Month'] = chirsp_df['Date'].dt.month\n",
    "\n",
    "climatology_vars = ['precipitation_MEAN']\n",
    "chirsp_climatology = chirsp_df.groupby(['Region', 'Month'])[climatology_vars].mean().reset_index()\n",
    "chirsp_merged = chirsp_df.merge(chirsp_climatology, on=['Region', 'Month'], suffixes=('', '_clim'))\n",
    "for var in climatology_vars:\n",
    "    chirsp_merged[f'{var}_anomaly'] = chirsp_merged[var] - chirsp_merged[f'{var}_clim']\n",
    "chirsp_anamoly = chirsp_merged.groupby(['Region', 'Date'])[\n",
    "    [f'{var}_anomaly' for var in climatology_vars]\n",
    "].mean().reset_index()\n",
    "\n",
    "regions = chirsp_anamoly['Region'].unique()\n",
    "for var in climatology_vars:\n",
    "    for region in regions:\n",
    "        subset = chirsp_anamoly[chirsp_anamoly['Region'] == region]\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        sns.lineplot(data=subset, x='Date', y=f'{var}_anomaly')\n",
    "        plt.title(f\"{var.capitalize()} Anomaly in {region} (1993–2023)\")\n",
    "        plt.ylabel(f\"{var.capitalize()} Anomaly\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save to FIGURE_DIR\n",
    "        filename = f\"{var}_anomaly_{region.replace(' ', '_')}.png\"\n",
    "        plt.savefig(os.path.join(FIGURE_DIR, filename), dpi=300)\n",
    "        plt.close()  # Close to avoid memory issues when looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure the datetime column is in datetime format\n",
    "updated_era_1993_2023['Date'] = pd.to_datetime(updated_era_1993_2023['Date'])\n",
    "\n",
    "# Count occurrences of each ID_2\n",
    "id_counts = updated_era_1993_2023['ID_2'].value_counts().sort_index()\n",
    "\n",
    "# Get the expected (most common) count\n",
    "expected_count = id_counts.mode()[0]\n",
    "\n",
    "# Get IDs that occur less than expected\n",
    "underrepresented_ids = id_counts[id_counts < expected_count].index\n",
    "\n",
    "# Filter the dataframe to just those IDs\n",
    "filtered_df = updated_era_1993_2023[updated_era_1993_2023['ID_2'].isin(underrepresented_ids)]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(filtered_df['Date'], filtered_df['ID_2'], alpha=0.6, s=10, color='red')\n",
    "plt.title('Occurrences of Underrepresented ID_2 Values Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('ID_2')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure the datetime column is in datetime format\n",
    "era_1993_2023['Date'] = pd.to_datetime(era_1993_2023['Date'])\n",
    "\n",
    "# Count occurrences of each ID_2\n",
    "id_counts = era_1993_2023['ID_2'].value_counts().sort_index()\n",
    "\n",
    "# Get the expected (most common) count\n",
    "expected_count = id_counts.mode()[0]\n",
    "\n",
    "# Get IDs that occur less than expected\n",
    "underrepresented_ids = id_counts[id_counts < expected_count].index\n",
    "\n",
    "# Filter the dataframe to just those IDs\n",
    "filtered_df = era_1993_2023[era_1993_2023['ID_2'].isin(underrepresented_ids)]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(filtered_df['Date'], filtered_df['ID_2'], alpha=0.6, s=10, color='red')\n",
    "plt.title('Occurrences of Underrepresented ID_2 Values Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('ID_2')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Count the number of occurrences of each ID_2\n",
    "id_counts = prec['ID_2'].value_counts().sort_index()\n",
    "\n",
    "# Determine the expected count (most common count)\n",
    "expected_count = id_counts.mode()[0]\n",
    "\n",
    "# Find ID_2s with counts less than the expected\n",
    "underrepresented_ids = id_counts[id_counts < expected_count]\n",
    "\n",
    "# Display the result\n",
    "print(\"Expected count per ID_2:\", expected_count)\n",
    "print(\"IDs with less than expected count:\")\n",
    "print(underrepresented_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Count the number of occurrences of each ID_2\n",
    "id_counts = updated_era_1993_2023['ID_2'].value_counts().sort_index()\n",
    "\n",
    "# Determine the expected count (most common count)\n",
    "expected_count = id_counts.mode()[0]\n",
    "\n",
    "# Find ID_2s with counts less than the expected\n",
    "underrepresented_ids = id_counts[id_counts < expected_count]\n",
    "\n",
    "# Display the result\n",
    "print(\"Expected count per ID_2:\", expected_count)\n",
    "print(\"IDs with less than expected count:\")\n",
    "print(underrepresented_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_2010_2023 = era_2010_2023.drop(columns=['total_evaporation_sum_MEAN','total_precipitation_sum_MEAN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merging era_1993_1999 with era_2005_2009, selecting only relevant columns\n",
    "merged_df1 = pd.merge(era_1993_1999, era_2005_2009[['Date', 'ID_2', 'Doy', 'potential_evaporation_sum_MEAN']],\n",
    "                      on=['Date', 'ID_2', 'Doy'], how='outer')\n",
    "\n",
    "# Merging the result with era5_temp_2000_2009, selecting only relevant columns\n",
    "final_df = pd.merge(merged_df1, era5_temp_2000_2009[['Date', 'ID_2', 'Doy', 'temperature_2m_MEAN', \n",
    "                                                     'temperature_2m_min_MEAN', 'temperature_2m_max_MEAN']],\n",
    "                     on=['Date', 'ID_2', 'Doy'], how='outer')\n",
    "\n",
    "# Fill missing values from columns with the other column values where necessary\n",
    "final_df['temperature_2m_MEAN'] = final_df['temperature_2m_MEAN_x'].fillna(final_df['temperature_2m_MEAN_y'])\n",
    "final_df['temperature_2m_min_MEAN'] = final_df['temperature_2m_min_MEAN_x'].fillna(final_df['temperature_2m_min_MEAN_y'])\n",
    "final_df['temperature_2m_max_MEAN'] = final_df['temperature_2m_max_MEAN_x'].fillna(final_df['temperature_2m_max_MEAN_y'])\n",
    "final_df['potential_evaporation_sum_MEAN'] = final_df['potential_evaporation_sum_MEAN_x'].fillna(final_df['potential_evaporation_sum_MEAN_y'])\n",
    "\n",
    "# Drop the old '_x' and '_y' columns\n",
    "final_df = final_df.drop(columns=['temperature_2m_MEAN_x', 'temperature_2m_MEAN_y', \n",
    "                                  'temperature_2m_min_MEAN_x', 'temperature_2m_min_MEAN_y',\n",
    "                                  'temperature_2m_max_MEAN_x', 'temperature_2m_max_MEAN_y',\n",
    "                                  'potential_evaporation_sum_MEAN_x', 'potential_evaporation_sum_MEAN_y'])\n",
    "\n",
    "# Display the final dataframe\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_1993_2023 = pd.concat([final_df,era_2010_2023])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'Date' and count unique 'ID_2' for each 'Date'\n",
    "unique_id_count_per_date = era_1993_2023.groupby('Date')['ID_2'].nunique()\n",
    "\n",
    "# Step 2: Check if the count of unique 'ID_2' for each 'Date' is 444\n",
    "incorrect_dates = unique_id_count_per_date[unique_id_count_per_date != 444]\n",
    "\n",
    "# Step 3: Output the result\n",
    "incorrect_dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize null values for precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column to check for null values\n",
    "column_to_check = 'precipitation_MEAN'\n",
    "\n",
    "# Create an empty dictionary to store plots for each year\n",
    "for year, group in prec.groupby('Year'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Count the number of null values for each day in the column\n",
    "    null_count = group[column_to_check].isnull().groupby(group['Date']).sum()\n",
    "    \n",
    "    # Plot the null count\n",
    "    plt.plot(null_count.index, null_count, label=column_to_check, color='blue')\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(f\"Null Value Count (Precipitation) for Each Day in {year}\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Null Value Count')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot for the year\n",
    "    save_path = f\"{save_dir}/nulls_per_day_{year}_precipitation.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()  # Close the plot to avoid displaying it inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check by ID, creating one bar plot for each year\n",
    "# Iterate over each year and create the plot\n",
    "for year, group in prec.groupby('Year'):\n",
    "    # Initialize a plot for the current year\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Initialize a list to store the null counts\n",
    "    null_counts_precipitation = []\n",
    "    id_2_values = []\n",
    "\n",
    "    # Iterate over each unique 'ID_2' and calculate the null counts\n",
    "    for id_2, sub_group in group.groupby('ID_2'):\n",
    "        # Count null values for 'precipitation_MEAN' for the current 'ID_2'\n",
    "        null_count_precipitation = sub_group['precipitation_MEAN'].isnull().sum()\n",
    "\n",
    "        # Store the null counts and 'ID_2'\n",
    "        null_counts_precipitation.append(null_count_precipitation)\n",
    "        id_2_values.append(id_2)\n",
    "\n",
    "    # Create the bar plot\n",
    "    width = 0.35  # Bar width\n",
    "    x = range(len(id_2_values))  # X positions for bars\n",
    "\n",
    "    # Bar plot for 'precipitation_MEAN'\n",
    "    plt.bar(x, null_counts_precipitation, width, label='precipitation_MEAN Nulls', color='blue')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('ID_2')\n",
    "    plt.ylabel('Null Value Count')\n",
    "    plt.title(f'Null Value Counts (Prec) for Each ID_2 in {year}')\n",
    "    plt.xticks([p + width / 2 for p in x], id_2_values, rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot for the current year\n",
    "    save_path = f\"{save_dir}/nulls_per_id_{year}_prec.png\"\n",
    "    plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()  # Close the plot to avoid displaying it inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise null values for NDVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns (NDVI_MEAN and EVI_MEAN)\n",
    "columns_to_check = ['NDVI_MEAN', 'EVI_MEAN']\n",
    "colors = ['red', 'blue']\n",
    "labels = ['NDVI_MEAN', 'EVI_MEAN']\n",
    "\n",
    "# Iterate over each year to create plots of time\n",
    "for year, group in ndvi_all.groupby('Year'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    null_percentages = {}  # Store null percentages for each column\n",
    "    total_counts = group['Date'].value_counts().sort_index()  # Total entries per day\n",
    "    \n",
    "    for column, color, label in zip(columns_to_check, colors, labels):\n",
    "        # Calculate null counts and convert to percentage\n",
    "        null_counts = group[column].isnull().groupby(group['Date']).sum()\n",
    "        null_percentages[column] = (null_counts / total_counts * 100).sort_index()\n",
    "        \n",
    "        # Plot each percentage curve\n",
    "        plt.plot(null_percentages[column].index, null_percentages[column], label=label, color=color)\n",
    "\n",
    "    # Highlight overlapping regions where both columns have the same percentage of missing data\n",
    "    if 'NDVI_MEAN' in null_percentages and 'EVI_MEAN' in null_percentages:\n",
    "        overlap = null_percentages['NDVI_MEAN'] == null_percentages['EVI_MEAN']\n",
    "        plt.fill_between(null_percentages['NDVI_MEAN'].index, null_percentages['NDVI_MEAN'], \n",
    "                         where=overlap, color='gray', alpha=0.3, label=\"Overlap\")\n",
    "\n",
    "    # Update plot title, labels, and legend\n",
    "    plt.title(f\"Missing Data Percentage (NDVI & EVI) for Each Day in {year}\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Missing Data Percentage (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Force y-axis to always go from 0 to 100%\n",
    "    plt.ylim(0, 100)\n",
    "\n",
    "    # Save the plot\n",
    "    save_path = f\"{save_dir}/nulls_percentage_per_day_{year}_ndvi.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check by ID, creating one bar plot for each year\n",
    "# Iterate over each year and create the plot\n",
    "for year, group in ndvi_all.groupby('Year'):\n",
    "    # Initialize a plot for the current year\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Initialize lists to store null counts and ID_2 values\n",
    "    null_counts = []\n",
    "    id_2_values = []\n",
    "\n",
    "    # Iterate over each unique 'ID_2' and calculate the null counts\n",
    "    for id_2, sub_group in group.groupby('ID_2'):\n",
    "        # Count null values for 'NDVI_MEAN' (since EVI_MEAN is redundant)\n",
    "        null_count = sub_group['EVI_MEAN'].isnull().sum()\n",
    "\n",
    "        # Store the null count and 'ID_2'\n",
    "        null_counts.append(null_count)\n",
    "        id_2_values.append(id_2)\n",
    "\n",
    "    # Create the bar plot\n",
    "    plt.bar(id_2_values, null_counts, color='red', label='Null Counts (NDVI & EVI)')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('ID_2')\n",
    "    plt.ylabel('Null Value Count')\n",
    "    plt.title(f'Null Value Counts for Each ID_2 in {year}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot for the current year\n",
    "    save_path = f\"{save_dir}/nulls_per_id_{year}_ndvi_evi.png\"\n",
    "    plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()  # Close the plot to avoid displaying it inline\n",
    "\n",
    "print('Barplot complete, generating choropleth maps.')\n",
    "\n",
    "\n",
    "# Because there are a fair few IDs with missing value, create choropleth to better visualise spatiality\n",
    "# Iterate over each year and create the choropleth map\n",
    "for year, group in ndvi_all.groupby('Year'):\n",
    "    null_counts_df = group.groupby('ID_2')['NDVI_MEAN'].apply(lambda x: x.isnull().sum()).reset_index()\n",
    "    \n",
    "    # Merge with shapefile\n",
    "    merged = in_shp.merge(null_counts_df, on='ID_2', how='left')\n",
    "    \n",
    "    # Initialize a plot for the current year\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    merged.plot(column='NDVI_MEAN', cmap='OrRd', linewidth=0.8, edgecolor='black', legend=True, ax=ax)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title(f'Choropleth Map of Null Counts for NDVI_MEAN in {year}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Save the plot for the current year\n",
    "    save_path = f\"{save_dir}/choropleth_nulls_{year}_NDVI_MEAN.png\"\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print('Choropleth maps complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for LST analysis\n",
    "columns_to_check = ['LST_Day_MEAN', 'LST_Night_MEAN', 'LST_Mean_MEAN']\n",
    "colors = ['red', 'blue', 'green']\n",
    "labels = ['LST Day', 'LST Night', 'LST Mean']\n",
    "\n",
    "# Iterate over each year to create plots of time\n",
    "for year, group in lst.groupby('Year'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    null_percentages = {}  # Store null percentages for each column\n",
    "    total_counts = group['Date'].value_counts().sort_index()  # Total entries per day\n",
    "    \n",
    "    for column, color, label in zip(columns_to_check, colors, labels):\n",
    "        # Calculate null counts and convert to percentage\n",
    "        null_counts = group[column].isnull().groupby(group['Date']).sum()\n",
    "        null_percentages[column] = (null_counts / total_counts * 100).sort_index()\n",
    "        \n",
    "        # Plot each percentage curve\n",
    "        plt.plot(null_percentages[column].index, null_percentages[column], label=label, color=color)\n",
    "\n",
    "    # Highlight overlapping regions where all columns have the same percentage of missing data\n",
    "    if len(columns_to_check) == 3:\n",
    "        overlap = (null_percentages['LST_Day_MEAN'] == null_percentages['LST_Night_MEAN']) & \\\n",
    "                  (null_percentages['LST_Day_MEAN'] == null_percentages['LST_Mean_MEAN'])\n",
    "        plt.fill_between(null_percentages['LST_Day_MEAN'].index, null_percentages['LST_Day_MEAN'], \n",
    "                         where=overlap, color='gray', alpha=0.3, label=\"Full Overlap\")\n",
    "\n",
    "    # Update plot title, labels, and legend\n",
    "    plt.title(f\"Missing Data Percentage (LST) for Each Day in {year}\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Missing Data Percentage (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Force y-axis to always go from 0 to 100%\n",
    "    plt.ylim(0, 100)\n",
    "\n",
    "    # Save the plot\n",
    "    save_path = f\"{save_dir}/nulls_percentage_per_day_{year}_lst.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns of interest\n",
    "columns_to_check = ['LST_Day_MEAN', 'LST_Night_MEAN', 'LST_Mean_MEAN']\n",
    "\n",
    "# Iterate over each year and create the plot for each column\n",
    "for year, group in lst.groupby('Year'):\n",
    "    for col in columns_to_check:\n",
    "        # Initialize a plot for the current year and column\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Initialize lists to store the null counts for each ID_2\n",
    "        null_counts = []\n",
    "        id_2_values = []\n",
    "\n",
    "        # Iterate over each unique 'ID_2' and calculate the null counts\n",
    "        for id_2, sub_group in group.groupby('ID_2'):\n",
    "            null_counts.append(sub_group[col].isnull().sum())\n",
    "            id_2_values.append(id_2)\n",
    "\n",
    "        # Create the bar plot\n",
    "        plt.bar(id_2_values, null_counts, color='blue')\n",
    "        \n",
    "        # Add labels and title\n",
    "        plt.xlabel('ID_2')\n",
    "        plt.ylabel('Null Value Count')\n",
    "        plt.title(f'Null Value Counts for {col} in {year}')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Save the plot for the current year and column\n",
    "        save_path = f\"{save_dir}/nulls_per_id_{year}_{col}.png\"\n",
    "        plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()  # Close the plot to avoid displaying it inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_100['ID_2'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Class_10', 'Class_20', 'Class_30', 'Class_40', 'Class_50', \n",
    "           'Class_60', 'Class_70', 'Class_80', 'Class_90', 'Class_95']\n",
    "\n",
    "# Plot setup\n",
    "ax = lulc_200.set_index('ID_2')[classes].plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(24, 6),\n",
    "    colormap='tab10',\n",
    "    edgecolor='none',\n",
    "    width=1  )\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Percentage of Area')\n",
    "plt.xlabel('Region (ID_2)')\n",
    "plt.title('Land Use/Land Cover Composition by Region')\n",
    "plt.legend(title='Class', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(\n",
    "    ticks=range(0, len(lulc_200['ID_2']), 5),  # Set ticks every 5 entries\n",
    "    labels=lulc_200['ID_2'][::5],  # Show only every 5th label\n",
    "    rotation=45\n",
    ")\n",
    "plt.tight_layout()\n",
    "save_path = f\"{save_dir}/lulc_200.png\"\n",
    "plt.savefig(save_path, dpi=600)\n",
    "plt.close()  # Close the plot to avoid displaying it inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shp['NAME_1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shp['Region'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumatra = ['Aceh', 'Bangka-Belitung', 'Bengkulu', 'Jambi', 'Kepulauan Riau', 'Lampung', 'Sumatera Barat', 'Sumatera Selatan', 'Sumatera Utara' ]\n",
    "nusa_tenggara = ['Bali', 'Nusa Tenggara Barat', 'Nusa Tenggara Timur']\n",
    "java = ['Banten', 'Jawa Barat', 'Jawa Tengah', 'Jawa Timur', 'Riau', 'Yogyakarta', 'Jakarta Raya']\n",
    "sulawesi = ['Gorontalo','Bengkulu', 'Sulawesi Barat', 'Sulawesi Selatan', 'Sulawesi Tengah', 'Sulawesi Tenggara', 'Sulawesi Utara']\n",
    "kalimantan = ['Kalimantan Barat', 'Kalimantan Selatan', 'Kalimantan Tengah', 'Kalimantan Timur', 'Kalimantan Utara']\n",
    "maluku_islands = ['Maluku Utara', 'Maluku']\n",
    "papua = ['Papua', 'Irian Jaya Barat']\n",
    "\n",
    "# Create a region lookup dictionary\n",
    "region_mapping = {province: 'Sumatra' for province in sumatra}\n",
    "region_mapping.update({province: 'Nusa Tenggara' for province in nusa_tenggara})\n",
    "region_mapping.update({province: 'Java' for province in java})\n",
    "region_mapping.update({province: 'Sulawesi' for province in sulawesi})\n",
    "region_mapping.update({province: 'Kalimantan' for province in kalimantan})\n",
    "region_mapping.update({province: 'Maluku Islands' for province in maluku_islands})\n",
    "region_mapping.update({province: 'Papua' for province in papua})\n",
    "\n",
    "# Assign regions to a new column based on NAME_1\n",
    "in_shp['Region'] = in_shp['NAME_1'].map(region_mapping).fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shp.to_file(r'D:\\Projects\\TMU\\gee_dengue\\Dengue_IN\\in_shp\\in_shp_islands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''irian jaya barat => papua barat\n",
    "Jakarta Raya => Jakarta Special Capital Region'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for LST analysis\n",
    "columns_to_check = ['LST_Day_MEAN', 'LST_Night_MEAN', 'LST_Mean_MEAN']\n",
    "colors = ['red', 'blue', 'green']\n",
    "labels = ['LST Day', 'LST Night', 'LST Mean']\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "null_percentages = {col: [] for col in columns_to_check}\n",
    "all_dates = []\n",
    "\n",
    "# Process each year and collect data\n",
    "for year, group in lst.groupby('Year'):\n",
    "    # Ensure Date is a proper datetime format\n",
    "    group['Date'] = pd.to_datetime(group['Date'], errors='coerce')\n",
    "    group = group.dropna(subset=['Date'])  # Drop rows with invalid dates\n",
    "    total_counts = group['Date'].value_counts().sort_index()\n",
    "    all_dates.extend(group['Date'].sort_values().unique())\n",
    "\n",
    "    for column in columns_to_check:\n",
    "        # Calculate null counts and convert to percentage\n",
    "        null_counts = group[column].isnull().groupby(group['Date']).sum()\n",
    "        percentages = (null_counts / total_counts * 100).sort_index()\n",
    "        null_percentages[column].extend(percentages)\n",
    "    \n",
    "    # Add a red line to separate years\n",
    "    plt.axvline(x=len(all_dates) - 1, color='red', linestyle='--', linewidth=1, label=f'Separator {year}')\n",
    "\n",
    "# Ensure dates are sorted correctly\n",
    "all_dates = sorted(pd.to_datetime(all_dates))\n",
    "\n",
    "# Apply smoothing using a rolling average\n",
    "window_size = 7  # 7-day smoothing\n",
    "for column, color, label in zip(columns_to_check, colors, labels):\n",
    "    smoothed_percentages = pd.Series(null_percentages[column]).rolling(window=window_size, min_periods=1).mean()\n",
    "    plt.plot(all_dates, smoothed_percentages, label=label, color=color)\n",
    "\n",
    "# Update plot title, labels, and legend\n",
    "plt.title(\"Missing Data Percentage (LST) Across All Years\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Missing Data Percentage (%)')\n",
    "plt.legend()\n",
    "\n",
    "# Force y-axis to always go from 0 to 100%\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Save the plot\n",
    "save_path = f\"{save_dir}/nulls_percentage_all_years_lst.png\"\n",
    "plt.savefig(save_path)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise null values for ERA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise null values for ERA by ID\n",
    "# Define the columns to check for null values\n",
    "columns_to_check = [\n",
    "    'temperature_2m',\n",
    "    'temperature_2m_min',\n",
    "    'temperature_2m_max',\n",
    "    'potential_evaporation_sum',\n",
    "    'total_evaporation_sum',\n",
    "    'total_precipitation_sum']\n",
    "\n",
    "# Iterate over each year and create the plot\n",
    "for year, group in era.groupby('Year'):\n",
    "    # Initialize a plot for the current year\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Initialize lists to store the null counts for each column\n",
    "    null_counts = {col: [] for col in columns_to_check}\n",
    "    id_2_values = []\n",
    "\n",
    "    # Iterate over each unique 'ID_2' and calculate the null counts\n",
    "    for id_2, sub_group in group.groupby('ID_2'):\n",
    "        # For each column, count the null values for the current 'ID_2'\n",
    "        for col in columns_to_check:\n",
    "            null_counts[col].append(sub_group[col].isnull().sum())\n",
    "        id_2_values.append(id_2)\n",
    "\n",
    "    # Create the bar plot\n",
    "    width = 0.15  # Bar width\n",
    "    x = range(len(id_2_values))  # X positions for bars\n",
    "\n",
    "    # Plot bars for each column of interest\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown']  # Colors for the bars\n",
    "    for idx, col in enumerate(columns_to_check):\n",
    "        plt.bar([p + width * idx for p in x], null_counts[col], width, label=col, color=colors[idx])\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('ID_2')\n",
    "    plt.ylabel('Null Value Count')\n",
    "    plt.title(f'Null Value Counts for Each ID_2 in {year}')\n",
    "    plt.xticks([p + width * (len(columns_to_check) / 2 - 0.5) for p in x], id_2_values, rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot for the current year\n",
    "    save_path = f\"{save_dir}/nulls_per_id_{year}_era.png\"\n",
    "    plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()  # Close the plot to avoid displaying it inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = [\n",
    "    'LST_Day_MEAN',\n",
    "    'LST_Night_MEAN',\n",
    "    'LST_Mean_MEAN']\n",
    "\n",
    "# Create an empty dictionary to store plots\n",
    "for year, group in lst.groupby('Year'):\n",
    "    # Count the number of null values per day in each column\n",
    "    null_count_day = group['LST_Day_MEAN'].isnull().groupby(group['Date']).sum()\n",
    "    null_count_night = group['LST_Night_MEAN'].isnull().groupby(group['Date']).sum()\n",
    "    null_count_mean = group['LST_Mean_MEAN'].isnull().groupby(group['Date']).sum()\n",
    "\n",
    "    # Plotting the null values for each day of the year\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(null_count_day.index, null_count_day, label='LST_Day_MEAN Nulls', color='red')\n",
    "    plt.plot(null_count_night.index, null_count_night, label='LST_Night_MEAN Nulls', color='blue')\n",
    "    plt.plot(null_count_mean.index, null_count_mean, label='LST_Mean_MEAN Nulls', color='green')\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title(f\"Null Value Count (LST) for Each Day in {year}\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Null Value Count')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot for the year\n",
    "    save_path = f\"{save_dir}/nulls_per_day_{year}_lst.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()  # Close the plot to avoid displaying it inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = r\"D:\\Projects\\TMU\\tn_dengue\\Graphs_2\"\n",
    "\n",
    "# Make sure 'Date' is a datetime type\n",
    "era['Date'] = pd.to_datetime(lst['Date'])\n",
    "\n",
    "# Extract the year from the 'Date' column\n",
    "era['Year'] = era['Date'].dt.year\n",
    "\n",
    "# Create an empty dictionary to store plots\n",
    "for year, group in era.groupby('Year'):\n",
    "    # Count the number of null values per day in each column\n",
    "    null_count_day = group['LST_Day_MEAN'].isnull().groupby(group['Date']).sum()\n",
    "    null_count_night = group['LST_Night_MEAN'].isnull().groupby(group['Date']).sum()\n",
    "    null_count_mean = group['LST_Mean_MEAN'].isnull().groupby(group['Date']).sum()\n",
    "\n",
    "    # Plotting the null values for each day of the year\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(null_count_day.index, null_count_day, label='LST_Day_MEAN Nulls', color='red')\n",
    "    plt.plot(null_count_night.index, null_count_night, label='LST_Night_MEAN Nulls', color='blue')\n",
    "    plt.plot(null_count_mean.index, null_count_mean, label='LST_Mean_MEAN Nulls', color='green')\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title(f\"Null Value Count for Each Day in {year}\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Null Value Count')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot for the year\n",
    "    save_path = f\"{save_dir}/nulls_per_day_{year}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()  # Close the plot to avoid displaying it inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_excel_sheets(excel_path):\n",
    "    '''Function to load in whole excel sheet, load in as individual dataframes, and rename columns.'''\n",
    "    excel_file = pd.ExcelFile(excel_path)\n",
    "    sheet_names = excel_file.sheet_names\n",
    "    \n",
    "    if len(sheet_names) > 14:\n",
    "        raise ValueError(\"Excel file has more than 14 sheets, cannot map to 2010-2023 range.\")\n",
    "    \n",
    "    new_names = [f\"in_{year}\" for year in range(2010, 2010 + len(sheet_names))]\n",
    "    \n",
    "    dataframes = {new_name: excel_file.parse(sheet_name=old_name) for old_name, new_name in zip(sheet_names, new_names)}\n",
    "\n",
    "    for idx, (name, df) in enumerate(dataframes.items()):\n",
    "        year = 2010 + idx  # Compute the corresponding year dynamically\n",
    "\n",
    "        rename_dict = {\n",
    "            3: 'Infection_1', 4: 'Death_1',\n",
    "            5: 'Infection_2', 6: 'Death_2',\n",
    "            7: 'Infection_3', 8: 'Death_3',\n",
    "            9: 'Infection_4', 10: 'Death_4',\n",
    "            11: 'Infection_5', 12: 'Death_5',\n",
    "            13: 'Infection_6', 14: 'Death_6',\n",
    "            15: 'Infection_7', 16: 'Death_7',\n",
    "            17: 'Infection_8', 18: 'Death_8',\n",
    "            19: 'Infection_9', 20: 'Death_9',\n",
    "            21: 'Infection_10', 22: 'Death_10',\n",
    "            23: 'Infection_11', 24: 'Death_11',\n",
    "            25: 'Infection_12', 26: 'Death_12',\n",
    "            27: f'Total_Infections_{year}', 28: f'Total_Death_{year}', 29: f'Incidence_Rate_{year}', 31: f'Population_{year}' \n",
    "        }\n",
    "\n",
    "        # Rename columns dynamically\n",
    "        df.columns = [rename_dict[i] if i in rename_dict else col for i, col in enumerate(df.columns)]\n",
    "        \n",
    "        # Drop the first row (index 0)\n",
    "        df.drop(index=0, inplace=True)\n",
    "        \n",
    "        # Create a global variable with the name corresponding to the sheet\n",
    "        globals()[name] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_excel_sheets(indonesia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''In this cell we create GeoDataframe that contains both polygon information and also incidence data.'''\n",
    "\n",
    "# Initialize an empty dictionary to store merged DataFrames\n",
    "merged_all = {}\n",
    "\n",
    "# List of DataFrame names (in_2010 to in_2023)\n",
    "years = range(2010, 2024)\n",
    "\n",
    "# Loop through each year, process the DataFrame, and store the result\n",
    "for year in years:\n",
    "    # Dynamically generate the DataFrame name (e.g., in_2010, in_2011, ..., in_2023)\n",
    "    df_name = f'in_{year}'\n",
    "    \n",
    "    # Get the corresponding DataFrame (e.g., in_2010, in_2011)\n",
    "    df = globals()[df_name]\n",
    "    \n",
    "    # Perform the merge with in_shp (shp remains the base)\n",
    "    merged = pd.merge(in_shp, df, \n",
    "                      left_on=['NAME_1', 'NAME_2', 'ENGTYPE_2'], \n",
    "                      right_on=['Province', 'City', 'City/Regency'], \n",
    "                      how='left')\n",
    "\n",
    "    # Store the merged DataFrame in the dictionary\n",
    "    merged_all[year] = merged\n",
    "    \n",
    "    # Also store it as a global variable (merged_2010, merged_2011, etc.)\n",
    "    globals()[f'merged_{year}'] = merged\n",
    "    \n",
    "    # Optional: Print the result to check the first few rows\n",
    "    print(f'Merged data for {year}:')\n",
    "    print(merged.head())\n",
    "\n",
    "# Now, merged DataFrames exist globally AND in merged_all dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Date' is in datetime format\n",
    "lst['Date'] = pd.to_datetime(lst['Date'])\n",
    "\n",
    "# Extract the year from the 'Date' column\n",
    "lst['Year'] = lst['Date'].dt.year\n",
    "\n",
    "# Define the columns of interest\n",
    "columns_to_check = ['LST_Day_MEAN', 'LST_Night_MEAN', 'LST_Mean_MEAN']\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over each year and create the plot for each column\n",
    "for year, group in lst.groupby('Year'):\n",
    "    null_counts_df = group.groupby('ID_2')[columns_to_check].apply(lambda x: x.isnull().sum()).reset_index()\n",
    "    \n",
    "    # Merge with shapefile\n",
    "    merged = in_shp.merge(null_counts_df, on='ID_2', how='left')\n",
    "    \n",
    "    for col in columns_to_check:\n",
    "        # Initialize a plot for the current year and column\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "        merged.plot(column=col, cmap='OrRd', linewidth=0.8, edgecolor='black', legend=True, ax=ax)\n",
    "        \n",
    "        # Add labels and title\n",
    "        plt.title(f'Choropleth Map of Null Counts for {col} in {year}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Save the plot for the current year and column\n",
    "        save_path = f\"{save_dir}/choropleth_nulls_{year}_{col}.png\"\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Date' is in datetime format\n",
    "era_1993_2023['Date'] = pd.to_datetime(era_1993_2023['Date'])\n",
    "prec['Date'] = pd.to_datetime(prec['Date'])\n",
    "# Extract the year from the 'Date' column\n",
    "era_1993_2023['Year'] = era_1993_2023['Date'].dt.year\n",
    "prec['Year'] = prec['Date'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Merge dataframes on ID_2\n",
    "merged = in_shp.merge(era_1993_2023, on='ID_2')\n",
    "\n",
    "# # Ensure Date column is datetime type and extract Year\n",
    "# merged['Date'] = pd.to_datetime(merged['Date'])\n",
    "# merged['Year'] = merged['Date'].dt.year\n",
    "\n",
    "# Select relevant statistic columns\n",
    "stat_columns = [\n",
    "    'temperature_2m_MEAN', 'temperature_2m_min_MEAN', 'temperature_2m_max_MEAN', 'potential_evaporation_sum_MEAN']\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(\"output_maps\", exist_ok=True)\n",
    "\n",
    "# Loop through each statistic\n",
    "for stat in stat_columns:\n",
    "    stat_name = stat.split('_Dropdown')[0]\n",
    "    years = merged['Year'].unique()\n",
    "    n_years = len(years)\n",
    "    n_cols = 4\n",
    "    n_rows = -(-n_years // n_cols)  # Ceiling division\n",
    "\n",
    "    # Create a grid of subplots with 4 columns and rows as needed\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop through each year and plot on the respective subplot\n",
    "    for i, year in enumerate(years):\n",
    "        yearly_data = merged[merged['Year'] == year]\n",
    "\n",
    "        # Aggregate statistics by region (ID_2)\n",
    "        summary = yearly_data.groupby('ID_2')[stat_columns].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "        # Flatten column names\n",
    "        summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "\n",
    "        # Merge back with shapefile\n",
    "        mapped_data = in_shp.merge(summary, left_on='ID_2', right_on='ID_2_')\n",
    "\n",
    "        metric_col = f\"{stat}_mean\"\n",
    "\n",
    "        # Plotting the map on the respective subplot\n",
    "        mapped_data.plot(\n",
    "            column=metric_col,\n",
    "            cmap='Blues',\n",
    "            linewidth=0.8,\n",
    "            ax=axes[i],\n",
    "            edgecolor='0.8',\n",
    "            legend=True\n",
    "        )\n",
    "\n",
    "        axes[i].set_title(f\"{stat_name} (Mean) - {year}\", fontsize=14)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Turn off any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    # Adjust layout and save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"output_maps/{stat_name}_Yearly_Grid.png\", dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "print(\"All statistic-based grid maps have been generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Assuming 'prec' is the new dataframe\n",
    "# Merge dataframes on ID_2\n",
    "merged = in_shp.merge(prec, on='ID_2')\n",
    "regions = merged['Region'].unique()\n",
    "\n",
    "# Select the relevant statistic column\n",
    "stat_column = 'precipitation_MEAN'\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(\"output_maps\", exist_ok=True)\n",
    "\n",
    "for region in regions:\n",
    "    # Filter data for the current region\n",
    "    region_data = merged[merged['Region'] == region]\n",
    "    region_shp = in_shp[in_shp['Region'] == region]\n",
    "    \n",
    "    # Get unique years for the region\n",
    "    years = region_data['Year'].unique()\n",
    "    n_years = len(years)\n",
    "    n_cols = 4\n",
    "    n_rows = -(-n_years // n_cols)  # Ceiling division\n",
    "\n",
    "    # Create a grid of subplots with 4 columns and rows as needed\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop through each year and plot on the respective subplot\n",
    "    for i, year in enumerate(years):\n",
    "        yearly_data = region_data[region_data['Year'] == year]\n",
    "\n",
    "        # Aggregate statistics by region (ID_2)\n",
    "        summary = yearly_data.groupby('ID_2')[stat_column].agg(['mean']).reset_index()\n",
    "\n",
    "        # Flatten column names\n",
    "        summary.columns = ['ID_2', f'{stat_column}_mean']\n",
    "\n",
    "        # Merge back with region shapefile\n",
    "        mapped_data = region_shp.merge(summary, on='ID_2')\n",
    "\n",
    "        # Plotting the map on the respective subplot\n",
    "        mapped_data.plot(\n",
    "            column=f'{stat_column}_mean',\n",
    "            cmap='Blues',\n",
    "            linewidth=0.8,\n",
    "            ax=axes[i],\n",
    "            edgecolor='0.8',\n",
    "            legend=True\n",
    "        )\n",
    "\n",
    "        axes[i].set_title(f\"{region} - {stat_column.split('_')[0]} (Mean) - {year}\", fontsize=14)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Turn off any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    # Adjust layout and save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"output_maps/{region}_{stat_column.split('_')[0]}_Yearly_Grid.png\", dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "print(\"All region-specific statistic-based grid maps have been generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Assuming 'prec' is the new dataframe\n",
    "# Merge dataframes on ID_2\n",
    "merged = in_shp.merge(prec, on='ID_2')\n",
    "\n",
    "# Select the relevant statistic column\n",
    "stat_column = 'precipitation_MEAN'\n",
    "\n",
    "# Aggregate the values of the statistic for each region\n",
    "region_aggregated = merged.groupby('Region')[stat_column].mean().reset_index()\n",
    "region_aggregated.columns = ['Region', f'{stat_column}_mean']\n",
    "\n",
    "# Merge the aggregated values back into the shapefile\n",
    "in_shp_aggregated = in_shp.merge(region_aggregated, on='Region')\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(\"output_maps\", exist_ok=True)\n",
    "\n",
    "# Plotting the entire in_shp with aggregated values for each region\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "in_shp_aggregated.plot(\n",
    "    column=f'{stat_column}_mean',\n",
    "    cmap='Blues',\n",
    "    linewidth=0.8,\n",
    "    ax=ax,\n",
    "    edgecolor='0.8',\n",
    "    legend=True\n",
    ")\n",
    "\n",
    "ax.set_title(f\"All Regions - {stat_column.split('_')[0]} (Mean) Aggregated\", fontsize=14)\n",
    "ax.axis('off')\n",
    "\n",
    "# Save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"output_maps/All_Regions_{stat_column.split('_')[0]}_Aggregated.png\", dpi=100)\n",
    "plt.close()\n",
    "\n",
    "print(\"The aggregated map of all regions has been generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regionwise maps\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Merge dataframes on ID_2\n",
    "merged = in_shp.merge(era_1993_2023, on='ID_2')\n",
    "regions = merged['Region'].unique()\n",
    "\n",
    "# Select relevant statistic columns\n",
    "stat_columns = [\n",
    "    'temperature_2m_MEAN', 'temperature_2m_min_MEAN', 'temperature_2m_max_MEAN', 'potential_evaporation_sum_MEAN']\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(\"output_maps\", exist_ok=True)\n",
    "\n",
    "for region in regions:\n",
    "    # Filter data for the current region\n",
    "    region_data = merged[merged['Region'] == region]\n",
    "    region_shp = in_shp[in_shp['Region'] == region]\n",
    "    \n",
    "    for stat in stat_columns:\n",
    "        stat_name = stat.split('_Dropdown')[0]\n",
    "        years = region_data['Year'].unique()\n",
    "        n_years = len(years)\n",
    "        n_cols = 4\n",
    "        n_rows = -(-n_years // n_cols)  # Ceiling division\n",
    "\n",
    "        # Create a grid of subplots with 4 columns and rows as needed\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Loop through each year and plot on the respective subplot\n",
    "        for i, year in enumerate(years):\n",
    "            yearly_data = region_data[region_data['Year'] == year]\n",
    "\n",
    "            # Aggregate statistics by region (ID_2)\n",
    "            summary = yearly_data.groupby('ID_2')[stat_columns].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "            # Flatten column names\n",
    "            summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "\n",
    "            # Merge back with region shapefile\n",
    "            mapped_data = region_shp.merge(summary, left_on='ID_2', right_on='ID_2_')\n",
    "\n",
    "            metric_col = f\"{stat}_mean\"\n",
    "\n",
    "            # Plotting the map on the respective subplot\n",
    "            mapped_data.plot(\n",
    "                column=metric_col,\n",
    "                cmap='Blues',\n",
    "                linewidth=0.8,\n",
    "                ax=axes[i],\n",
    "                edgecolor='0.8',\n",
    "                legend=True\n",
    "            )\n",
    "\n",
    "            axes[i].set_title(f\"{region} - {stat_name} (Mean) - {year}\", fontsize=14)\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        # Turn off any unused subplots\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "\n",
    "        # Adjust layout and save figure\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"output_maps/{region}_{stat_name}_Yearly_Grid.png\", dpi=100)\n",
    "        plt.close()\n",
    "\n",
    "print(\"All region-specific statistic-based grid maps have been generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Assuming 'prec' is the new dataframe\n",
    "# Merge dataframes on ID_2\n",
    "merged = in_shp.merge(prec, on='ID_2')\n",
    "\n",
    "# Select the relevant statistic column\n",
    "stat_column = 'precipitation_MEAN'\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(\"output_maps\", exist_ok=True)\n",
    "\n",
    "# Get unique years\n",
    "years = merged['Year'].unique()\n",
    "\n",
    "for year in years:\n",
    "    # Filter data for the current year\n",
    "    year_data = merged[merged['Year'] == year]\n",
    "    \n",
    "    # Calculate the standard deviation for each region\n",
    "    region_std = year_data.groupby('Region')[stat_column].std().reset_index()\n",
    "    region_std.columns = ['Region', f'{stat_column}_std']\n",
    "\n",
    "    # Merge the standard deviation values back into the shapefile\n",
    "    in_shp_std = in_shp.merge(region_std, on='Region')\n",
    "\n",
    "    # Plotting the entire in_shp with standard deviation values for each region\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    in_shp_std.plot(\n",
    "        column=f'{stat_column}_std',\n",
    "        cmap='Blues',\n",
    "        linewidth=0.8,\n",
    "        ax=ax,\n",
    "        edgecolor='0.8',\n",
    "        legend=True\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"All Regions - {stat_column.split('_')[0]} Standard Deviation - {year}\", fontsize=14)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"output_maps/All_Regions_{stat_column.split('_')[0]}_Std_{year}.png\", dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "print(\"Standard deviation maps for all regions and years have been generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of every polygon in a given region, then plot whole region as one\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Merge dataframes on ID_2\n",
    "merged = in_shp.merge(era_1993_2023, on='ID_2')\n",
    "\n",
    "# Extract unique regions from the merged dataframe\n",
    "regions = merged['Region'].unique()\n",
    "\n",
    "# Select relevant statistic columns\n",
    "stat_columns = [\n",
    "    'temperature_2m_MEAN', 'temperature_2m_min_MEAN', 'temperature_2m_max_MEAN', 'potential_evaporation_sum_MEAN']\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(\"output_maps\", exist_ok=True)\n",
    "\n",
    "# Loop through each statistic\n",
    "for stat in stat_columns:\n",
    "    stat_name = stat.split('_Dropdown')[0]\n",
    "    years = merged['Year'].unique()\n",
    "    n_years = len(years)\n",
    "    n_cols = 4\n",
    "    n_rows = -(-n_years // n_cols)  # Ceiling division\n",
    "\n",
    "    # Create a grid of subplots with 4 columns and rows as needed\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop through each year and plot on the respective subplot\n",
    "    for i, year in enumerate(years):\n",
    "        yearly_data = merged[merged['Year'] == year]\n",
    "\n",
    "        # Aggregate statistics by Region (calculate mean of all polygons per region)\n",
    "        region_summary = yearly_data.groupby('Region')[stat_columns].mean().reset_index()\n",
    "\n",
    "        # Merge back with the full shapefile\n",
    "        mapped_data = in_shp.merge(region_summary, on='Region')\n",
    "\n",
    "        metric_col = f\"{stat}\"\n",
    "\n",
    "        # Plot the whole map with each region as one unit\n",
    "        mapped_data.plot(\n",
    "            column=metric_col,\n",
    "            cmap='Blues',\n",
    "            linewidth=0.15,\n",
    "            ax=axes[i],\n",
    "            edgecolor='0.8',\n",
    "            legend=True\n",
    "        )\n",
    "\n",
    "        axes[i].set_title(f\"{stat_name} (Region Mean) - {year}\", fontsize=14)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Turn off any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    # Adjust layout and save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"output_maps/{stat_name}_Region_Aggregated_Yearly_Grid.png\", dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "print(\"All region-aggregated statistic-based grid maps have been generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes on ID_2\n",
    "merged = in_shp.merge(prec, on='ID_2')\n",
    "\n",
    "# Ensure Date column is datetime type and extract Year\n",
    "merged['Date'] = pd.to_datetime(merged['Date'])\n",
    "merged['Year'] = merged['Date'].dt.year\n",
    "\n",
    "# Define the relevant column\n",
    "stat_column = 'precipitation_MEAN'\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(\"output_maps\", exist_ok=True)\n",
    "\n",
    "# Loop through each year\n",
    "for year in merged['Year'].unique():\n",
    "    yearly_data = merged[merged['Year'] == year]\n",
    "\n",
    "    # Aggregate statistics by region (ID_2)\n",
    "    summary = yearly_data.groupby('ID_2')[stat_column].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    summary.rename(columns={'mean': 'precipitation_mean', 'std': 'precipitation_std'}, inplace=True)\n",
    "\n",
    "    # Merge back with shapefile\n",
    "    mapped_data = in_shp.merge(summary, on='ID_2')\n",
    "\n",
    "    # Generate plots for mean and standard deviation\n",
    "    for metric in ['mean', 'std']:\n",
    "        metric_col = f\"precipitation_{metric}\"\n",
    "        title_metric = \"Mean\" if metric == \"mean\" else \"Standard Deviation\"\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        mapped_data.plot(column=metric_col, cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
    "        ax.set_title(f\"Precipitation {title_metric} - {year}\", fontsize=15)\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Save figure\n",
    "        plt.savefig(f\"output_maps/precipitation_{metric}_{year}.png\", dpi=100)\n",
    "        plt.close()\n",
    "\n",
    "print(\"All maps have been generated successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee_dengue_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
